# ğŸŒŠ LakeStream

<p align="center">
  <strong>B2B Web Scraping & Data Extraction Platform</strong><br>
  Built for speed, built for scale, built for your business. ğŸš€
</p>

<p align="center">
  <a href="https://python.org"><img src="https://img.shields.io/badge/Python-3.12+-blue?logo=python" alt="Python"></a>
  <a href="https://github.com/Champ-Deep/LakeStream/blob/main/LICENSE"><img src="https://img.shields.io/badge/License-MIT-green" alt="License"></a>
  <a href="https://github.com/Champ-Deep/LakeStream/actions"><img src="https://img.shields.io/badge/Tests-Passing-brightgreen" alt="Tests"></a>
  <a href="https://discord.gg/lakestream"><img src="https://img.shields.io/badge/Community-Discord-purple" alt="Discord"></a>
</p>

---

## âœ¨ What is LakeStream?

LakeStream is a **powerful B2B web scraping platform** that extracts valuable data from any website â€” blogs, articles, pricing pages, contact info, tech stacks, and more! 

Whether you're an **SEO team** monitoring competitors or a **data team** building enrichment pipelines, LakeStream handles the heavy lifting so you can focus on insights. ğŸ’¡

---

## âš¡ Quick Start

```bash
# 1ï¸âƒ£ Install dependencies
pip install -r requirements.txt

# 2ï¸âƒ£ Start infrastructure (PostgreSQL + Redis)
make docker-up

# 3ï¸âƒ£ Run the API & worker
make dev          # API on http://localhost:3001
make worker       # Background job processor
```

**Boom!** You're ready to scrape. ğŸ‰

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         LakeStream                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚   FastAPI   â”‚â”€â”€â”€â”€â–¶â”‚  Job Queue   â”‚â”€â”€â”€â”€â–¶â”‚   Workers    â”‚  â”‚
â”‚   â”‚     API     â”‚     â”‚    (arq)     â”‚     â”‚   (async)    â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â”‚                                           â”‚           â”‚
â”‚         â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚           â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  PostgreSQL  â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                        â”‚   Database   â”‚                       â”‚
â”‚                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚                               â”‚                                 â”‚
â”‚                               â–¼                                 â”‚
â”‚                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚                        â”‚     n8n      â”‚                        â”‚
â”‚                        â”‚ Enrichment   â”‚                        â”‚
â”‚                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ”¥ Three-Tier Adaptive Scraping

| Tier | What It Does | Best For |
|------|--------------|----------|
| ğŸŒ **Basic HTTP** | Lightning fast requests | Simple pages, APIs |
| ğŸ•µï¸ **Headless Browser** | Renders JavaScript | SPAs, dynamic content |
| ğŸ›¡ï¸ **Stealth + Proxy** | Bypasses protection | Cloudflare, protected sites |

LakeStream **automatically escalates** between tiers when it detects blocks, CAPTCHAs, or empty responses. No manual intervention needed! 

---

## ğŸ¯ Use Cases

### For SEO Teams ğŸ”

> "I need to monitor my competitors' content strategy."

```python
# Scrape competitor blog posts
from src.services.scraper import ScraperService

scraper = ScraperService()
result = await scraper.scrape("https://competitor.com/blog")

print(result["title"])        # Blog post title
print(result["markdown"])     # Full content in Markdown
print(result["metadata"])     # Author, date, tags
```

| ğŸ¯ Task | ğŸ’¼ How LakeStream Helps |
|---------|------------------------|
| **Competitor Blogging** | Monitor posting frequency, topics, engagement |
| **Content Gaps** | Find topics competitors cover that you don't |
| **Pricing Monitoring** | Track competitor pricing pages in real-time |
| **Backlink Analysis** | Discover who's linking to competitors |
| **Site Audits** | Extract all pages for technical SEO analysis |

### For Data Teams ğŸ“Š

> "I need to build B2B enrichment pipelines."

```python
# Enrich company data at scale
from src.services.scraper import ScraperService
from src.models.scraping import ScrapingTier

# Scrape with specific tier
scraper = ScraperService()
result = await scraper.scrape(
    "https://techcompany.com/about",
    tier=ScrapingTier.HEADLESS_BROWSER
)

# Extract structured data
print(result["markdown"])     # Clean Markdown
print(result["metadata"])     # JSON with title, description, etc.
```

| ğŸ¯ Task | ğŸ’¼ How LakeStream Helps |
|---------|------------------------|
| **Lead Generation** | Extract contact info, job titles, company data |
| **Tech Stack Detection** | Identify tools/tech used on any site |
| **Market Research** | Scrape industry blogs, news, resources |
| **Data Enrichment** | Fill gaps in existing databases |
| **API Alternative** | Get data when APIs don't exist |

---

## ğŸ”§ Configuration

Create a `.env` file:

```bash
# Database
DATABASE_URL=postgresql://user:pass@localhost:5432/lakedb

# Redis
REDIS_URL=redis://localhost:6379

# ğŸ” Proxy (optional - for Tier 3)
BRIGHTDATA_PROXY_URL=
SMARTPROXY_URL=

# ğŸ” LakeCurrent (search discovery)
LAKECURRENT_BASE_URL=http://localhost:8001

# ğŸ”‘ Authentication
JWT_SECRET=your-secret-key
```

---

## ğŸ“š API Examples

### Scrape a Single URL

```python
import asyncio
from src.services.scraper import ScraperService

async def main():
    scraper = ScraperService()
    result = await scraper.scrape("https://example.com")
    
    print(result["markdown"])  # Clean content
    print(result["metadata"])  # {title, author, date, ...}

asyncio.run(main())
```

### Discover Domains from Search

```python
from src.services.lakecurrent import LakeCurrentClient

client = LakeCurrentClient(
    base_url="http://localhost:8001",
    timeout=15.0
)
results = await client.search("B2B SaaS companies", limit=10)

for r in results.results:
    print(r.url, r.title)
```

### Map All URLs on a Domain

```python
from src.services.crawler import CrawlerService

crawler = CrawlerService()
urls = await crawler.map_domain("https://example.com", limit=100)

print(f"Found {len(urls)} URLs")
```

---

## ğŸ§ª Testing

```bash
# Run all tests
make test

# Run specific test
pytest tests/unit/scraping/test_lake_fetcher.py -v

# Run with coverage
pytest --cov=src tests/
```

---

## ğŸ“Š Benchmarking

Compare tier performance:

```bash
python benchmarks/scrapling_benchmark.py https://example.com https://python.org
```

---

## ğŸ”Œ API Endpoints

### Web Dashboard
| Endpoint | Description |
|----------|-------------|
| `GET /` | Main dashboard |
| `GET /jobs` | Job list |
| `GET /results` | Browse extracted data |
| `GET /domains` | Domain analytics |

### REST API
| Endpoint | Method | Description |
|----------|--------|-------------|
| `/api/scrape/execute` | POST | Create scrape job |
| `/api/scrape/status/{job_id}` | GET | Job status |
| `/api/discover/search` | POST | Search-driven discovery |
| `/api/export/csv/{job_id}` | GET | Export as CSV |
| `/api/health` | GET | Health check |

---

## ğŸ¤ Contributing

1. ğŸ´ Fork the repo
2. ğŸŒ¿ Create a branch (`git checkout -b feature/amazing`)
3. ğŸ’» Make your changes
4. âœ… Run tests (`make test`)
5. ğŸ“ Commit with clear messages
6. ğŸš€ Submit a PR

---

## ğŸ“„ License

**MIT License** â€” See [LICENSE](LICENSE) for details.

---

## ğŸ™ Acknowledgments

- ğŸ Python community â€” For making this all possible
- ğŸ’œ **You** â€” For choosing LakeStream!

---

<p align="center">
  <sub>Built with â¤ï¸ by <a href="https://lakeb2b.com">Lake B2B</a></sub>
</p>
